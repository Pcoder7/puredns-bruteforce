name: Parallel PureDNS Bruteforce

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  LINES_PER_CHUNK: 500000
  COMBINED_WORDLIST: "best-wordlist-9996122_filter.txt"
 

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
      unique_domains_json: ${{ steps.set_matrix.outputs.unique_domains_json }}
      all_chunk_basenames_json: ${{ steps.set_matrix.outputs.all_chunk_basenames_json }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Cache Go modules and binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install prerequisites
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y git make gcc dos2unix jq
          # go is provided by setup-go

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'

      - name: Install massdns & PureDNS
        run: |
          if ! command -v massdns >/dev/null; then
            git clone https://github.com/blechschmidt/massdns.git
            cd massdns && make && sudo make install && cd ..
          else
            echo "massdns already installed"
          fi        
          if ! command -v puredns >/dev/null; then
            go install github.com/d3mondev/puredns/v2@latest
          else
            echo "puredns already installed"
          fi    
          if ! command -v anew >/dev/null; then
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already installed"
          fi          

          echo "$HOME/go/bin" >> $GITHUB_PATH
      
      - name: Cache Wordlists
        uses: actions/cache@v3
        with:
          path: |
  
            best-wordlist-9996122.txt
            seclistes-dns-7110368.txt
            resolvers.txt
          key: wordlists-cache-v2-${{ hashFiles('best-wordlist-9996122.txt', 'seclistes-dns-7110368.txt', 'resolvers.txt') }} # More specific cache key
          restore-keys: |
            wordlists-cache-v2-
          
      - name: Fetch wordlists
        run: |
          # Download only if not cached already
          if [ ! -f best-wordlist-9996122.txt ]; then
            wget -qO best-wordlist-9996122.txt \
              https://github.com/Pcoder7/All-In-One-DNS-Wordlist/raw/refs/heads/main/best-wordlist-9996122.txt
          fi
          if [ ! -f seclistes-dns-7110368.txt ]; then
            wget -qO seclistes-dns-7110368.txt \
              https://github.com/Pcoder7/All-In-One-DNS-Wordlist/raw/refs/heads/main/seclistes-dns-7110368.txt
          fi
          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi

      - name: Build filter_wordlist tool
        run: |
          
          go build -o filter_wordlist filter_wordlist.go
          chmod +x filter_wordlist
  
      - name: Cache filtered wordlists
        uses: actions/cache@v3
        with:
          path: |
            best-wordlist-9996122_filter.txt
            seclistes-dns-7110368_filter.txt
          key: filter-wordlists-${{ hashFiles('best-wordlist-9996122_filter.txt', 'seclistes-dns-7110368_filter.txt') }}
          restore-keys: |
            filter-wordlists-
                 
      - name: Filter and combine wordlists
        run: |
          # If filtered wordlist is already cached, skip filtering
          if [ -f best-wordlist-9996122_filter.txt ]; then
            echo "Cached filtered wordlist found—skipping filter_wordlist."
          else
            if [ -f ./filter_wordlist ]; then
              echo "Filtering wordlists..."
              ./filter_wordlist best-wordlist-9996122.txt > best-wordlist-9996122_filter.txt
              ./filter_wordlist seclistes-dns-7110368.txt > seclistes-dns-7110368_filter.txt
      
              cat seclistes-dns-7110368_filter.txt \
                | anew -q best-wordlist-9996122_filter.txt > added-lines.txt
              echo "Lines added to combined list from seclistes:"
              wc -l added-lines.txt
            else
              echo "filter_wordlist tool not found—using raw wordlists."
              if [ ! -f "$COMBINED_WORDLIST" ] && [ -f "best-wordlist-9996122.txt" ]; then
                echo "Warning: $COMBINED_WORDLIST missing. Copying raw wordlist."
                cp best-wordlist-9996122.txt "$COMBINED_WORDLIST"
              elif [ ! -f "$COMBINED_WORDLIST" ]; then
                echo "Error: $COMBINED_WORDLIST cannot be found or created."
                exit 1
              fi
            fi
          fi    

      - name: Split combined wordlist into chunks
        run: |
          if [ ! -f "$COMBINED_WORDLIST" ]; then
            echo "Error: Combined wordlist '$COMBINED_WORDLIST' not found. Cannot split."
            exit 1
          fi
          mkdir -p chunks
          echo "Splitting '$COMBINED_WORDLIST' into chunks of $LINES_PER_CHUNK lines..."
          split -l $LINES_PER_CHUNK -a 2 --numeric-suffixes=1 "$COMBINED_WORDLIST" chunks/chunk_
          CHUNKS_COUNT=$(ls chunks | wc -l)
          echo "Generated $CHUNKS_COUNT chunks."
          if [ "$CHUNKS_COUNT" -eq 0 ]; then
            echo "Error: No chunks were generated. Wordlist might be empty or too small."
            exit 1
          fi
          echo "Ensuring placeholder files for artifact exist..."
          touch wildcards.txt massdns.txt # Create empty files if they don't exist

     
      - name: Build matrix JSON and define outputs
        id: set_matrix
        run: |
          if [ ! -f domains.txt ]; then
            echo "Error: domains.txt not found!"
            exit 1
          fi
          echo "Contents of domains.txt:"
          cat domains.txt
          echo "---"

          echo "Listing files in chunks/ directory:"
          ls -lA chunks/
          echo "---"
          
          doms_raw=$(grep -E '\S' domains.txt)
          echo "Debug: Raw domains from grep: '$doms_raw'"
          doms_jq_array=$(echo "$doms_raw" | jq -R -s -c 'split("\n") | map(select(length > 0))') # Safer map(select)
          echo "Debug: doms_jq_array is: $doms_jq_array"

          chunks_raw=$(ls chunks)
          echo "Debug: Raw chunks from ls: '$chunks_raw'"
          chunks_jq_array=$(echo "$chunks_raw" | jq -R -s -c 'split("\n") | map(select(length > 0))') # Safer map(select)
          echo "Debug: chunks_jq_array is: $chunks_jq_array"

          if [[ -z "$doms_jq_array" || "$doms_jq_array" == "[]" || -z "$chunks_jq_array" || "$chunks_jq_array" == "[]" ]]; then
            echo "Warning: Either domains or chunks array is empty or invalid. Matrix will likely be empty."
            # Decide if this should be an error:
            # exit 1
          fi

          full_matrix_jq=$(jq -n --argjson D "$doms_jq_array" --argjson C "$chunks_jq_array" \
            '[ $D[] as $d | $C[] as $c | {domain:$d,chunk:"chunks/\($c)"} ]' 2>/tmp/jq_error.log || true)
          
          if [ -s /tmp/jq_error.log ]; then
            echo "JQ error for full_matrix_jq:"
            cat /tmp/jq_error.log
          fi
          echo "Debug: full_matrix_jq is: $full_matrix_jq"

          # Crucial check: if full_matrix_jq is empty or not valid JSON, this is the problem
          if [[ -z "$full_matrix_jq" || "$full_matrix_jq" == "null" ]]; then
            echo "Error: full_matrix_jq is empty or null. Cannot set matrix output."
            # To ensure fromJson doesn't get an empty string, output an empty JSON array string explicitly
            full_matrix_jq="[]" 
            echo "Setting full_matrix_jq to '[]' to avoid fromJson error, but this indicates a problem upstream."
            # Consider exiting with error: exit 1
          fi
          
          echo "matrix<<EOF" >> $GITHUB_OUTPUT
          echo "$full_matrix_jq" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          # ... (rest of the script for unique_domains_json and all_chunk_basenames_json)
          # Make sure to also handle if full_matrix_jq is "[]" for these:
          if [[ "$full_matrix_jq" == "[]" ]]; then
            unique_domains_for_agg_jq="[]"
            # chunks_jq_array would already be set
          else
            unique_domains_for_agg_jq=$(echo "$full_matrix_jq" | jq 'map(.domain) | unique | map(select(. != null and . != ""))')
          fi
          echo "Debug: unique_domains_for_agg_jq is: $unique_domains_for_agg_jq"


          echo "unique_domains_json<<EOF" >> $GITHUB_OUTPUT
          echo "$unique_domains_for_agg_jq" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "all_chunk_basenames_json<<EOF" >> $GITHUB_OUTPUT
          echo "$chunks_jq_array" >> $GITHUB_OUTPUT # This should be fine as it's directly from ls
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Upload chunks as artifact
        uses: actions/upload-artifact@v4 
        with:
          name: common-assets
          path: |      
            chunks
            resolvers.txt 
            wildcards.txt # Initial empty wildcards file, puredns will append to it.
            massdns.txt   # Initial empty massdns file, puredns will append to it.
           

  bruteforce:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 20   
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Go # <-- ADD THIS STEP
        id: setup-go 
        uses: actions/setup-go@v5
        with:
          go-version: '1.22' # Or your desired Go version        
      
      - name: Restore Go cache
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
            filter_wordlist
          #key: ${{ runner.os }}-go-cache-${{ hashFiles('**/go.sum') }}
          
          key: ${{ runner.os }}-go-${{ steps.setup-go.outputs.go-version }}-${{ hashFiles('filter_wordlist.go') }}
          restore-keys: |
           ${{ runner.os }}-go-${{ steps.setup-go.outputs.go-version }}-
           # ${{ runner.os }}-go-cache-
            
      - name: Add Go bin to PATH
        run: echo "${{ env.HOME }}/go/bin" >> $GITHUB_PATH
        
      - name: Install prerequisites for massdns
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y git make gcc

      - name: Install massdns
        run: |
          if ! command -v massdns >/dev/null; then
            echo "massdns not found, installing..."
            git clone https://github.com/blechschmidt/massdns.git
            cd massdns
            make -j$(nproc)
            sudo make install
            cd ..
            rm -rf massdns
          else
            echo "massdns already installed or cached."
          fi
          # Verify puredns is available (from cache or needs install if cache failed)
          if ! command -v puredns >/dev/null; then
            echo "puredns not found in PATH after cache restore. Installing..."
            go install github.com/d3mondev/puredns/v2@latest
          else
            echo "puredns is available."
          fi
          if ! command -v anew >/dev/null; then
            echo "anew not found in PATH after cache restore. Installing..."   
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already installed"
          fi          

      - name: Download puredns assets (chunks and resolvers)
        uses: actions/download-artifact@v4 
        with:
          name: common-assets
          
                 
      - name: Run PureDNS Bruteforce on chunk
        run: |
         
          DOMAIN="${{ matrix.pair.domain }}"
          CHUNK_FILE="${{ matrix.pair.chunk }}" # Using the updated matrix key
          

          # Verify files downloaded correctly
          if [ ! -d "chunks" ]; then
            echo "Error: 'chunks' directory not found after artifact download!"
            ls -A . # List current directory contents
            exit 1
          fi
          if [ ! -f "$CHUNK_FILE" ]; then
            echo "Error: Chunk file '$CHUNK_FILE' not found after artifact download!"
            ls -A chunks/ # List chunks directory contents
            exit 1
          fi
          if [ ! -f "resolvers.txt" ]; then
            echo "Error: resolvers.txt not found after artifact download!"
            ls -A .
            exit 1
          fi
          
          # Sanitize domain for use in output file names if needed
          D_FILENAME=$(echo "$DOMAIN" | tr '.' '_')
          OUT_DIR="puredns_temp_output_for_artifact" 

            OUT_DIR="puredns_temp_output_for_artifact" # Using a distinct name for clarity
          mkdir -p "$OUT_DIR"
          CHUNK_BASENAME=$(basename "$CHUNK_FILE") 
          # These file paths are now relative to the temporary OUT_DIR
          OUT_FILE="$OUT_DIR/results_${CHUNK_BASENAME}.txt"
          WILDCARDS_FILE="$OUT_DIR/wildcards_${CHUNK_BASENAME}.txt"
          MASSDNS_FILE="$OUT_DIR/massdns_${CHUNK_BASENAME}.txt"



          # Correctly derive basename from CHUNK_FILE


          echo "⇢ Bruteforcing $DOMAIN with $CHUNK_FILE"
          echo "Outputting results to $OUT_FILE"
          
          # The puredns command itself. User requested not to change this.
          # Ensure the output files are correctly handled.
          # puredns will create these files.
          cat "$CHUNK_FILE" | puredns bruteforce "$DOMAIN" \
            -r resolvers.txt \
            --write "$OUT_FILE" \
            --write-wildcards "$WILDCARDS_FILE" \
            --write-massdns "$MASSDNS_FILE"
          
          echo "Puredns scan complete for $DOMAIN with $CHUNK_FILE."
          echo "Results are in $OUT_FILE"
           # Ensure files exist for upload, even if empty
          if [ -f "$OUT_FILE" ]; then echo "Results in $OUT_FILE"; else touch "$OUT_FILE"; fi
          if [ -f "$WILDCARDS_FILE" ]; then echo "Wildcards in $WILDCARDS_FILE"; else touch "$WILDCARDS_FILE"; fi
          if [ -f "$MASSDNS_FILE" ]; then echo "MassDNS in $MASSDNS_FILE"; else touch "$MASSDNS_FILE"; fi

          # Set outputs for the artifact upload step
          echo "D_FILENAME_FOR_ARTIFACT=$D_FILENAME" >> $GITHUB_ENV
          echo "CHUNK_BASENAME_FOR_ARTIFACT=$CHUNK_BASENAME" >> $GITHUB_ENV
          echo "TEMP_OUT_DIR_FOR_ARTIFACT=$OUT_DIR" >> $GITHUB_ENV

      - name: Upload individual chunk results
        uses: actions/upload-artifact@v4
        with:
          name: artifact_${{ env.D_FILENAME_FOR_ARTIFACT }}_${{ env.CHUNK_BASENAME_FOR_ARTIFACT }}
          path: |
            ${{ env.TEMP_OUT_DIR_FOR_ARTIFACT }}/results_${{ env.CHUNK_BASENAME_FOR_ARTIFACT }}.txt
            ${{ env.TEMP_OUT_DIR_FOR_ARTIFACT }}/wildcards_${{ env.CHUNK_BASENAME_FOR_ARTIFACT }}.txt
            ${{ env.TEMP_OUT_DIR_FOR_ARTIFACT }}/massdns_${{ env.CHUNK_BASENAME_FOR_ARTIFACT }}.txt

          

  aggregate_results:
    needs: [prepare_matrix, bruteforce] # Depends on both previous jobs
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # matrix.domain_from_prepare_matrix will be like "example.com"
        # Using 'unique_domains_json' output from 'prepare_matrix' job
        domain_from_prepare_matrix: ${{ fromJson(needs.prepare_matrix.outputs.unique_domains_json) }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Need full history for rebase

      - name: Install JQ
        run: sudo apt-get update -qq && sudo apt-get install -y jq

      - name: Prepare directories and define key variables
        id: prep_vars
        run: |
          # This is the actual domain string, e.g., "example.com"
          CURRENT_DOMAIN="${{ matrix.domain_from_prepare_matrix }}"
          echo "Processing domain: $CURRENT_DOMAIN"

          # This is the sanitized version for filenames/paths, e.g., "example_com"
          # This matches D_FILENAME_FOR_ARTIFACT from the bruteforce job
          DOMAIN_FILENAME_SAFE=$(echo "$CURRENT_DOMAIN" | tr '.' '_')
          echo "Sanitized domain filename: $DOMAIN_FILENAME_SAFE"
          
          # Directory in the repository for final aggregated files
          # e.g., results/example_com
          FINAL_REPO_DIR="results/$DOMAIN_FILENAME_SAFE"
          mkdir -p "$FINAL_REPO_DIR"

          # Temporary local directory to gather this domain's chunk files after downloading all artifacts
          # e.g., temp_domain_files/example_com
          TEMP_FILES_GATHER_DIR="temp_domain_files/$DOMAIN_FILENAME_SAFE"
          mkdir -p "$TEMP_FILES_GATHER_DIR"

          echo "current_domain_env=$CURRENT_DOMAIN" >> $GITHUB_ENV
          echo "domain_filename_safe_env=$DOMAIN_FILENAME_SAFE" >> $GITHUB_ENV
          echo "final_repo_dir_env=$FINAL_REPO_DIR" >> $GITHUB_ENV
          echo "temp_files_gather_dir_env=$TEMP_FILES_GATHER_DIR" >> $GITHUB_ENV

      - name: Download all workflow artifacts
        uses: actions/download-artifact@v4
        with:
          name: artifact_${{ env.D_FILENAME_FOR_ARTIFACT }}_${{ env.CHUNK_BASENAME_FOR_ARTIFACT }}
        # No 'name' specified, so it downloads ALL artifacts produced by the workflow.
        # Each artifact will be in its own directory named after the artifact.
        # e.g., ./artifact_example_com_chunk_01/results_chunk_01.txt

      - name: Consolidate artifacts for current domain (${{ env.current_domain_env }})
        run: |
          echo "Consolidating artifacts for domain: ${{ env.current_domain_env }} (filename: ${{ env.domain_filename_safe_env }})"
          echo "Looking for artifact directories named like: artifact_${{ env.domain_filename_safe_env }}_*"
          echo "Copying their contents to: ${{ env.temp_files_gather_dir_env }}"
          
          found_artifacts_for_domain=false
          # Artifact directories are named like: artifact_D_FILENAME_FOR_ARTIFACT_CHUNK_BASENAME_FOR_ARTIFACT
          # So we search for artifact_${{ env.domain_filename_safe_env }}_*
          for artifact_source_dir in artifact_${{ env.domain_filename_safe_env }}_*; do
            if [ -d "$artifact_source_dir" ]; then
              echo "Processing downloaded artifact directory: $artifact_source_dir"
              # Copy all files from this artifact_source_dir into TEMP_FILES_GATHER_DIR
              # Using -n to not overwrite, though shouldn't be an issue with unique chunk names in file names
              cp -n "$artifact_source_dir"/* "${{ env.temp_files_gather_dir_env }}/" 2>/dev/null || true 
              found_artifacts_for_domain=true
            else
              echo "Debug: Expected artifact directory $artifact_source_dir not found (this is normal if loop is just checking possibilities)"
            fi
          done

          if [ "$found_artifacts_for_domain" = false ]; then
            echo "Warning: No artifact directories found matching 'artifact_${{ env.domain_filename_safe_env }}_*' for domain ${{ env.current_domain_env }}."
          else
            echo "Files in temporary gathering directory (${{ env.temp_files_gather_dir_env }}) after consolidation:"
            ls -lA "${{ env.temp_files_gather_dir_env }}"
          fi

      - name: Aggregate results for domain (${{ env.current_domain_env }})
        run: |
          shopt -s nullglob # Globs expand to nothing if no match

          # Source directory for individual chunk files for this domain
          # e.g., temp_domain_files/example_com
          SOURCE_CHUNK_FILES_DIR="${{ env.temp_files_gather_dir_env }}" 
          
          # Target directory for aggregated files in the git repo
          # e.g., results/example_com
          TARGET_AGGREGATED_DIR="${{ env.final_repo_dir_env }}"      

          echo "Aggregating files from $SOURCE_CHUNK_FILES_DIR into $TARGET_AGGREGATED_DIR for domain ${{ env.current_domain_env }}"

          # Aggregate results_chunk.txt
          # Files in SOURCE_CHUNK_FILES_DIR are named like results_chunk_01.txt, results_chunk_02.txt etc.
          RESULTS_FILES=("$SOURCE_CHUNK_FILES_DIR"/results_*.txt)
          if [ ${#RESULTS_FILES[@]} -gt 0 ]; then
            echo "Found ${#RESULTS_FILES[@]} results files to aggregate."
            cat "${RESULTS_FILES[@]}" | sort -u > "$TARGET_AGGREGATED_DIR/results_chunk.txt"
          else
            echo "No results_*.txt files found in $SOURCE_CHUNK_FILES_DIR for ${{ env.current_domain_env }}. Creating empty results_chunk.txt."
            touch "$TARGET_AGGREGATED_DIR/results_chunk.txt"
          fi

          # Aggregate massdns_chunk.txt
          MASSDNS_FILES=("$SOURCE_CHUNK_FILES_DIR"/massdns_*.txt)
          if [ ${#MASSDNS_FILES[@]} -gt 0 ]; then
            echo "Found ${#MASSDNS_FILES[@]} massdns files to aggregate."
            cat "${MASSDNS_FILES[@]}" | sort -u > "$TARGET_AGGREGATED_DIR/massdns_chunk.txt"
          else
            echo "No massdns_*.txt files found in $SOURCE_CHUNK_FILES_DIR for ${{ env.current_domain_env }}. Creating empty massdns_chunk.txt."
            touch "$TARGET_AGGREGATED_DIR/massdns_chunk.txt"
          fi

          # Aggregate wildcards_chunk.txt
          WILDCARDS_FILES=("$SOURCE_CHUNK_FILES_DIR"/wildcards_*.txt)
          if [ ${#WILDCARDS_FILES[@]} -gt 0 ]; then
            echo "Found ${#WILDCARDS_FILES[@]} wildcards files to aggregate."
            cat "${WILDCARDS_FILES[@]}" | sort -u > "$TARGET_AGGREGATED_DIR/wildcards_chunk.txt"
          else
            echo "No wildcards_*.txt files found in $SOURCE_CHUNK_FILES_DIR for ${{ env.current_domain_env }}. Creating empty wildcards_chunk.txt."
            touch "$TARGET_AGGREGATED_DIR/wildcards_chunk.txt"
          fi
          
          # Create check_chunk.txt
          CHECK_FILE_FINAL_PATH="$TARGET_AGGREGATED_DIR/check_chunk.txt"
          >"$CHECK_FILE_FINAL_PATH" # Create/clear the final check file
          
          # Using 'all_chunk_basenames_json' output from 'prepare_matrix' job
          ALL_CHUNK_BASENAMES_JSON='${{ needs.prepare_matrix.outputs.all_chunk_basenames_json }}'
          
          echo "Generating $CHECK_FILE_FINAL_PATH for domain ${{ env.current_domain_env }} using all_chunk_basenames_json: $ALL_CHUNK_BASENAMES_JSON"

          # Iterate over all possible chunk basenames defined in prepare_matrix
          echo "$ALL_CHUNK_BASENAMES_JSON" | jq -r '.[]' | while IFS= read -r each_chunk_basename; do
              # each_chunk_basename is like "chunk_01", "chunk_02"
              # Check for the existence of the corresponding result file for *this domain* and *this chunk*
              expected_chunk_result_file="$SOURCE_CHUNK_FILES_DIR/results_${each_chunk_basename}.txt"
              
              if [ -f "$expected_chunk_result_file" ]; then
                  # Count non-empty lines to determine if valid domains were found
                  line_count=$(grep -cvE '^\s*$' "$expected_chunk_result_file") 
                  if [ "$line_count" -gt 0 ]; then
                      printf "%s found %02d valid domains\n" "$each_chunk_basename" "$line_count" >> "$CHECK_FILE_FINAL_PATH"
                  else
                      printf "%s No valid domains found\n" "$each_chunk_basename" >> "$CHECK_FILE_FINAL_PATH"
                  fi
              else
                  # This means the results_*.txt file for this chunk_basename was not found for this domain.
                  # This could happen if a bruteforce job for a particular chunk of a domain failed,
                  # or if puredns didn't produce an output file for some reason.
                  printf "%s result file not found for this domain\n" "$each_chunk_basename" >> "$CHECK_FILE_FINAL_PATH"
              fi
          done
          
          if [ ! -s "$CHECK_FILE_FINAL_PATH" ]; then # if file is empty (e.g. all_chunk_basenames_json was empty)
              echo "No chunk processing information recorded for domain ${{ env.current_domain_env }}." > "$CHECK_FILE_FINAL_PATH"
          fi
          echo "Aggregation complete for domain ${{ env.current_domain_env }}. Aggregated files are in $TARGET_AGGREGATED_DIR"

      - name: Commit and push aggregated results for domain (${{ env.current_domain_env }})
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          # Add only the aggregated files for the current domain
          # TARGET_AGGREGATED_DIR is like results/example_com
          git add "${{ env.final_repo_dir_env }}/results_chunk.txt" \
                  "${{ env.final_repo_dir_env }}/massdns_chunk.txt" \
                  "${{ env.final_repo_dir_env }}/wildcards_chunk.txt" \
                  "${{ env.final_repo_dir_env }}/check_chunk.txt"

          if ! git diff --cached --quiet; then
            git commit -m "Aggregate puredns results for ${{ env.current_domain_env }}"
            
            MAX_ATTEMPTS=5
            ATTEMPT_NUM=1
            SUCCESS=false
            while [ $ATTEMPT_NUM -le $MAX_ATTEMPTS ]; do
              echo "Attempt $ATTEMPT_NUM to rebase and push for ${{ env.current_domain_env }}..."
              git fetch origin main 
              if git pull --rebase --autostash origin main; then
                if git push origin HEAD:main; then 
                  echo "Push successful for ${{ env.current_domain_env }} on attempt $ATTEMPT_NUM."
                  SUCCESS=true
                  break
                else
                  echo "Push failed for ${{ env.current_domain_env }} on attempt $ATTEMPT_NUM after rebase."
                fi
              else
                echo "Pull --rebase failed for ${{ env.current_domain_env }} on attempt $ATTEMPT_NUM."
              fi
              if [ $ATTEMPT_NUM -eq $MAX_ATTEMPTS ]; then
                echo "Max push attempts reached for ${{ env.current_domain_env }}. Giving up."
                break 
              fi
              sleep $((RANDOM % 5 + 5)) 
              ATTEMPT_NUM=$((ATTEMPT_NUM + 1))
            done
            if [ "$SUCCESS" = false ]; then
              echo "Failed to push aggregated results for ${{ env.current_domain_env }} after $MAX_ATTEMPTS attempts."
              exit 1 
            fi
          else
            echo "No changes to commit for aggregated results of ${{ env.current_domain_env }}."
          fi
      
      - name: Clean up downloaded artifacts and temp domain files
        if: always() 
        run: |
          echo "Cleaning up downloaded artifact directories (artifact_*) and temp domain files gathering dir (temp_domain_files)..."
          rm -rf artifact_* # Removes all directories starting with artifact_
          rm -rf temp_domain_files # Removes the parent temp directory
          echo "Cleanup complete."
 
