name: Parallel PureDNS Bruteforce

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  LINES_PER_CHUNK: 500000
  COMBINED_WORDLIST: "best-wordlist-9996122_filter.txt"
 

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Cache Go modules and binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Install prerequisites
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y git make gcc dos2unix
          # go is provided by setup-go

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'

      - name: Install massdns & PureDNS
        run: |
          if ! command -v massdns >/dev/null; then
            git clone https://github.com/blechschmidt/massdns.git
            cd massdns && make && sudo make install && cd ..
          else
            echo "massdns already installed"
          fi        
          if ! command -v puredns >/dev/null; then
            go install github.com/d3mondev/puredns/v2@latest
          else
            echo "puredns already installed"
          fi    
          if ! command -v anew >/dev/null; then
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already installed"
          fi          

          echo "$HOME/go/bin" >> $GITHUB_PATH
      
      - name: Cache Wordlists
        uses: actions/cache@v3
        with:
          path: |
  
            best-wordlist-9996122.txt
            seclistes-dns-7110368.txt
            resolvers.txt
          key: wordlists-cache-v2-${{ hashFiles('best-wordlist-9996122.txt', 'seclistes-dns-7110368.txt', 'resolvers.txt') }} # More specific cache key
          restore-keys: |
            wordlists-cache-v2-
          
      - name: Fetch wordlists
        run: |
          # Download only if not cached already
          if [ ! -f best-wordlist-9996122.txt ]; then
            wget -qO best-wordlist-9996122.txt \
              https://github.com/Pcoder7/All-In-One-DNS-Wordlist/raw/refs/heads/main/best-wordlist-9996122.txt
          fi
          if [ ! -f seclistes-dns-7110368.txt ]; then
            wget -qO seclistes-dns-7110368.txt \
              https://github.com/Pcoder7/All-In-One-DNS-Wordlist/raw/refs/heads/main/seclistes-dns-7110368.txt
          fi
          if [ ! -f resolvers.txt ]; then
              wget -qO resolvers.txt \
              https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
              echo "resolvers.txt is downloaded"
          fi

      - name: Build filter_wordlist tool
        run: |
          
          go build -o filter_wordlist filter_wordlist.go
          chmod +x filter_wordlist
  
      - name: Cache filtered wordlists
        uses: actions/cache@v3
        with:
          path: |
            best-wordlist-9996122_filter.txt
            seclistes-dns-7110368_filter.txt
          key: filter-wordlists-${{ hashFiles('best-wordlist-9996122_filter.txt', 'seclistes-dns-7110368_filter.txt') }}
          restore-keys: |
            filter-wordlists-
                 
      - name: Filter and combine wordlists
        run: |
          # If filtered wordlist is already cached, skip filtering
          if [ -f best-wordlist-9996122_filter.txt ]; then
            echo "Cached filtered wordlist foundâ€”skipping filter_wordlist."
          else
            if [ -f ./filter_wordlist ]; then
              echo "Filtering wordlists..."
              ./filter_wordlist best-wordlist-9996122.txt > best-wordlist-9996122_filter.txt
              ./filter_wordlist seclistes-dns-7110368.txt > seclistes-dns-7110368_filter.txt
      
              cat seclistes-dns-7110368_filter.txt \
                | anew -q best-wordlist-9996122_filter.txt > added-lines.txt
              echo "Lines added to combined list from seclistes:"
              wc -l added-lines.txt
            else
              echo "filter_wordlist tool not foundâ€”using raw wordlists."
              if [ ! -f "$COMBINED_WORDLIST" ] && [ -f "best-wordlist-9996122.txt" ]; then
                echo "Warning: $COMBINED_WORDLIST missing. Copying raw wordlist."
                cp best-wordlist-9996122.txt "$COMBINED_WORDLIST"
              elif [ ! -f "$COMBINED_WORDLIST" ]; then
                echo "Error: $COMBINED_WORDLIST cannot be found or created."
                exit 1
              fi
            fi
          fi    

      - name: Split combined wordlist into chunks
        run: |
          if [ ! -f "$COMBINED_WORDLIST" ]; then
            echo "Error: Combined wordlist '$COMBINED_WORDLIST' not found. Cannot split."
            exit 1
          fi
          mkdir -p chunks
          echo "Splitting '$COMBINED_WORDLIST' into chunks of $LINES_PER_CHUNK lines..."
          split -l $LINES_PER_CHUNK -a 2 --numeric-suffixes=1 "$COMBINED_WORDLIST" chunks/chunk_
          CHUNKS_COUNT=$(ls chunks | wc -l)
          echo "Generated $CHUNKS_COUNT chunks."
          if [ "$CHUNKS_COUNT" -eq 0 ]; then
            echo "Error: No chunks were generated. Wordlist might be empty or too small."
            exit 1
          fi
          echo "Ensuring placeholder files for artifact exist..."
          touch wildcards.txt massdns.txt # Create empty files if they don't exist

      - name: Build matrix JSON
        id: set_matrix
        run: |
          if [ ! -f domains.txt ]; then
            echo "Error: domains.txt not found!"
            exit 1
          fi
          # Ensure domains.txt is not empty and jq is installed
          sudo apt-get install -y jq
   
          doms=$(grep -E '\S' domains.txt | jq -R -s -c 'split("\n")[:-1]')
          chs=$(ls chunks | jq -R -s -c 'split("\n")[:-1]')
          matrix=$(jq -n --argjson D "$doms" --argjson C "$chs" \
            '[ $D[] as $d | $C[] as $c | {domain:$d,chunk:"chunks/\($c)"} ]')
          # Use multi-line output for GitHub Actions
          echo "matrix<<EOF" >> $GITHUB_OUTPUT
          echo "$matrix" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Upload chunks as artifact
        uses: actions/upload-artifact@v4 
        with:
          name: chunks
          path: |      
            chunks
            resolvers.txt 
            wildcards.txt # Initial empty wildcards file, puredns will append to it.
            massdns.txt   # Initial empty massdns file, puredns will append to it.
           

  bruteforce:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 20   
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Setup Go # <-- ADD THIS STEP
        id: setup-go 
        uses: actions/setup-go@v5
        with:
          go-version: '1.22' # Or your desired Go version        
      
      - name: Restore Go cache
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
            filter_wordlist
          #key: ${{ runner.os }}-go-cache-${{ hashFiles('**/go.sum') }}
          
          key: ${{ runner.os }}-go-${{ steps.setup-go.outputs.go-version }}-${{ hashFiles('filter_wordlist.go') }}
          restore-keys: |
           ${{ runner.os }}-go-${{ steps.setup-go.outputs.go-version }}-
           # ${{ runner.os }}-go-cache-
            
      - name: Add Go bin to PATH
        run: echo "${{ env.HOME }}/go/bin" >> $GITHUB_PATH
        
      - name: Install prerequisites for massdns
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y git make gcc

      - name: Install massdns
        run: |
          if ! command -v massdns >/dev/null; then
            echo "massdns not found, installing..."
            git clone https://github.com/blechschmidt/massdns.git
            cd massdns
            make -j$(nproc)
            sudo make install
            cd ..
            rm -rf massdns
          else
            echo "massdns already installed or cached."
          fi
          # Verify puredns is available (from cache or needs install if cache failed)
          if ! command -v puredns >/dev/null; then
            echo "puredns not found in PATH after cache restore. Installing..."
            go install github.com/d3mondev/puredns/v2@latest
          else
            echo "puredns is available."
          fi
          if ! command -v anew >/dev/null; then
            echo "anew not found in PATH after cache restore. Installing..."   
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already installed"
          fi          

      - name: Download puredns assets (chunks and resolvers)
        uses: actions/download-artifact@v4 
        with:
          name: chunks
          
      - name: Create results directory
        run: |
          D="${{ matrix.pair.domain }}"
          mkdir -p "results/$D"
                  
      - name: Run PureDNS Bruteforce on chunk
        run: |
         
          DOMAIN="${{ matrix.pair.domain }}"
          CHUNK_FILE="${{ matrix.pair.chunk }}" # Using the updated matrix key
          

          # Verify files downloaded correctly
          if [ ! -d "chunks" ]; then
            echo "Error: 'chunks' directory not found after artifact download!"
            ls -A . # List current directory contents
            exit 1
          fi
          if [ ! -f "$CHUNK_FILE" ]; then
            echo "Error: Chunk file '$CHUNK_FILE' not found after artifact download!"
            ls -A chunks/ # List chunks directory contents
            exit 1
          fi
          if [ ! -f "resolvers.txt" ]; then
            echo "Error: resolvers.txt not found after artifact download!"
            ls -A .
            exit 1
          fi
          
          # Sanitize domain for use in output file names if needed
          D_FILENAME=$(echo "$DOMAIN" | tr '.' '_')
          OUT_DIR="results/$D_FILENAME"
          
          # These output files are specific to puredns's operation here
          # Make massdns file unique per chunk/domain
          mkdir -p "$OUT_DIR" # Ensure output directory exists

          # Correctly derive basename from CHUNK_FILE
          CHUNK_BASENAME=$(basename "$CHUNK_FILE")

          OUT_FILE="$OUT_DIR/results_${CHUNK_BASENAME}.txt"
          # Corrected definitions for WILDCARDS_FILE and MASSDNS_FILE:
          WILDCARDS_FILE="$OUT_DIR/wildcards_${CHUNK_BASENAME}.txt"
          MASSDNS_FILE="$OUT_DIR/massdns_${CHUNK_BASENAME}.txt"
          echo "â‡¢ Bruteforcing $DOMAIN with $CHUNK_FILE"
          echo "Outputting results to $OUT_FILE"
          
          # The puredns command itself. User requested not to change this.
          # Ensure the output files are correctly handled.
          # puredns will create these files.
          cat "$CHUNK_FILE" | puredns bruteforce "$DOMAIN" \
            -r resolvers.txt \
            --write "$OUT_FILE" \
            --write-wildcards "$WILDCARDS_FILE" \
            --write-massdns "$MASSDNS_FILE"
          
          echo "Puredns scan complete for $DOMAIN with $CHUNK_FILE."
          echo "Results are in $OUT_FILE"
          if [ -f "$WILDCARDS_FILE" ]; then echo "Wildcards in $WILDCARDS_FILE"; fi
          if [ -f "$MASSDNS_FILE" ]; then echo "MassDNS formatted output in $MASSDNS_FILE"; fi

          # Set outputs for the artifact upload step
          # We upload the contents of ARTIFACT_STAGING_DIR
          echo "artifact_upload_path=$ARTIFACT_STAGING_DIR" >> $GITHUB_OUTPUT
          # Artifact name needs to be unique per chunk if multiple chunks for same domain run in parallel
          echo "artifact_upload_name=chunk_output_${S_DOMAIN}_${C_BASE}" >> $GITHUB_OUTPUT

      - name: Upload scan results for this chunk
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.puredns_run.outputs.artifact_upload_name }} # e.g., chunk_output_example_com_chunk_01
          path: ${{ steps.puredns_run.outputs.artifact_upload_path }} # Uploads contents of 'temp_for_upload'
                                                                    # This means it uploads the 'example_com' dir
                                                                    # containing 'results_chunk_01.txt' etc.
          retention-days: 1

          

  aggregate_results:
    needs: bruteforce
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: read
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
        with:
          ref: ${{ github.ref_name }}
          fetch-depth: 0

      - name: Download all chunk output artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./dl_artifacts # Each artifact in its own subdirectory

      - name: Prepare downloaded files for aggregation
        id: prep_files
        run: |
          echo "Preparing downloaded artifacts..."
          art_base="./dl_artifacts" # Base dir where artifacts were downloaded
          res_root="results"        # Root for final structured results (e.g., results/example_com)
          mkdir -p "$res_root"
          
          if [ ! -d "$art_base" ] || [ -z "$(ls -A $art_base)" ]; then
            echo "No artifacts found in $art_base. Nothing to prepare."
            echo "results_prepared=false" >> $GITHUB_OUTPUT
            exit 0 
          fi

          # Iterate through each downloaded artifact directory
          # e.g., ./dl_artifacts/chunk_output_example_com_chunk_01/
          find "$art_base" -mindepth 1 -maxdepth 1 -type d -name "chunk_output_*" | while read -r downloaded_artifact_dir; do
            # Inside downloaded_artifact_dir, there should be a single domain directory
            # e.g., ./dl_artifacts/chunk_output_example_com_chunk_01/example_com/
            
            # Find the domain directory *within* the current downloaded artifact directory
            # Assuming only one domain directory per artifact upload from bruteforce's ARTIFACT_STAGING_DIR
            find "$downloaded_artifact_dir" -mindepth 1 -maxdepth 1 -type d | while read -r source_domain_dir_in_artifact; do
                s_domain=$(basename "$source_domain_dir_in_artifact") # e.g., example_com
                
                echo "Prep > Found domain '$s_domain' in artifact '$downloaded_artifact_dir'"

                target_domain_dir_final="$res_root/$s_domain" # e.g., results/example_com
                mkdir -p "$target_domain_dir_final"

                # Copy all files (results_chunk_XX.txt, massdns_chunk_XX.txt, etc.)
                # from the source_domain_dir_in_artifact to the target_domain_dir_final.
                # Using 'cp -n' to avoid overwriting if somehow multiple artifacts tried to create the exact same file,
                # though artifact naming should make chunk files unique.
                # Or use 'cp -rT' if source_domain_dir_in_artifact might have subdirs (not expected here).
                # A simple 'cp *' assumes files only.
                echo "Copying files from $source_domain_dir_in_artifact to $target_domain_dir_final"
                if [ -n "$(ls -A "$source_domain_dir_in_artifact")" ]; then # Check if dir is not empty
                    cp "$source_domain_dir_in_artifact"/* "$target_domain_dir_final/"
                else
                    echo "Warn: Source domain directory $source_domain_dir_in_artifact is empty."
                fi
            done
          done
          
          echo "results_prepared=true" >> $GITHUB_OUTPUT
          echo "Preparation complete. Files should be in $res_root/DOMAIN_NAME/file_CHUNK_NAME.txt"

      - name: Aggregate perâ€‘domain outputs
        if: steps.prep_files.outputs.results_prepared == 'true'
        # THIS IS YOUR SCRIPT - IT SHOULD NOW WORK AS INTENDED
        # It expects results/DOMAIN_NAME/results_chunk_01.txt, results_chunk_02.txt etc.
        run: |
          echo "Starting aggregation..."
          if [ ! -d "results" ] || [ -z "$(ls -A results)" ]; then
            echo "âš ï¸ 'results' directory is missing or empty. Nothing to aggregate."
            exit 0
          fi
          for D_DIR_ABSOLUTE_PATH in results/*/; do
            if [ ! -d "$D_DIR_ABSOLUTE_PATH" ]; then
              echo "No domain subdirectories found in results/ or pattern did not match."
              break 
            fi
            _temp_domain_part=$(basename "$D_DIR_ABSOLUTE_PATH") 
            D=${_temp_domain_part} 
            D_DIR=$D_DIR_ABSOLUTE_PATH 
            echo "Agg > Domain: $D in dir: $D_DIR"
            if ! cd "$D_DIR"; then
                echo "Error: Could not change to directory $D_DIR. Skipping."
                continue 
            fi
            shopt -s nullglob
            chunks=(results_*.txt) 
            massdns_chunk_files=(massdns_*.txt) 
            wildcards_chunk_files=(wildcards_*.txt)
            if [ ${#chunks[@]} -eq 0 ]; then
              echo "âš ï¸ No results_*.txt files to merge in $(pwd) for domain $D"
            fi
            if [ ${#chunks[@]} -gt 0 ]; then cat "${chunks[@]}" > results_chunk.txt; else : > results_chunk.txt; fi
            if [ ${#massdns_chunk_files[@]} -gt 0 ]; then cat "${massdns_chunk_files[@]}" > massdns_chunk.txt; else : > massdns_chunk.txt; fi
            if [ ${#wildcards_chunk_files[@]} -gt 0 ]; then cat "${wildcards_chunk_files[@]}" > wildcards_chunk.txt; else : > wildcards_chunk.txt; fi
            : > check_chunk.txt 
            if [ ${#chunks[@]} -gt 0 ]; then 
              for f in "${chunks[@]}"; do 
                c_full_name=${f#results_}    
                c=${c_full_name%.txt}        
                if [ -f "$f" ]; then
                    count=$(wc -l < "$f")
                    if [ "$count" -gt 0 ]; then printf "%s Found %s domains\n" "$c" "$count"; else printf "%s No valid domains found\n" "$c"; fi >> check_chunk.txt
                fi
              done
            else
              echo "No results_*.txt files found for $D to generate specific check_chunk.txt details." >> check_chunk.txt
            fi
            cd .. 
          done 
          echo "Aggregation complete."

      - name: Commit & push aggregated results
        if: steps.prep_files.outputs.results_prepared == 'true'
        # ... (commit and push script remains the same, with retry loop) ...
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add results/*/{results_chunk.txt,massdns_chunk.txt,wildcards_chunk.txt,check_chunk.txt}
          if ! git diff --cached --quiet; then
            git commit -m "ðŸ“¦ Add consolidated puredns results for workflow run"
            MAX_ATTEMPTS=5
            ATTEMPT_NUM=1
            SUCCESS=false
            while [ $ATTEMPT_NUM -le $MAX_ATTEMPTS ]; do
              echo "Attempt $ATTEMPT_NUM to rebase and push aggregated results..."
              if git pull --rebase --autostash origin ${{ github.ref_name }}; then
                if git push origin HEAD:${{ github.ref_name }}; then
                  echo "Push successful on attempt $ATTEMPT_NUM."
                  SUCCESS=true
                  break
                else
                  echo "Push failed on attempt $ATTEMPT_NUM after rebase."
                fi
              else
                echo "Pull --rebase failed on attempt $ATTEMPT_NUM."
              fi
              if [ $ATTEMPT_NUM -eq $MAX_ATTEMPTS ]; then exit 1; fi
              sleep $((RANDOM % 5 + 5)) 
              ATTEMPT_NUM=$((ATTEMPT_NUM + 1))
            done
            if [ "$SUCCESS" = false ]; then exit 1; fi
          else
            echo "No changes to aggregated results to commit."
          fi
 
